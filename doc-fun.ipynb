{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9338a8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Œ Configuration set:\n",
      "   Topic: ParallelHDF5\n",
      "   Runs: 5\n",
      "   Model: gpt-4o-mini\n",
      "   Temperature: 0.7\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Configuration Variables\n",
    "# ============================================\n",
    "# CHANGE THESE VARIABLES TO CUSTOMIZE OUTPUT\n",
    "# ============================================\n",
    "\n",
    "TOPIC = \"ParallelHDF5\"  # <-- Change this! Examples: \"TensorFlow\", \"OpenMP\", \"MATLAB\", \"Conda\", \"Git\"\n",
    "RUNS = 5  # Number of variations to generate (1-10 recommended)\n",
    "MODEL = 'gpt-4o-mini'  # Options: 'gpt-4', 'gpt-3.5-turbo'\n",
    "TEMPERATURE = 0.2  # Creativity (0.0 = deterministic, 1.0 = very creative)\n",
    "\n",
    "# Query template - modify if you want different phrasing\n",
    "QUERY_TEMPLATE = \"Create a knowledge base article with regards to using {topic} on the FASRC cluster, using the tone of graduate level Academic Computing documentation.\"\n",
    "\n",
    "# Alternative query templates you can use:\n",
    "# QUERY_TEMPLATE = \"Generate HTML documentation for {topic} following academic computing standards\"\n",
    "# QUERY_TEMPLATE = \"Write a technical reference page for {topic} on HPC clusters\"\n",
    "# QUERY_TEMPLATE = \"Create a comprehensive guide for using {topic} in a research computing environment\"\n",
    "\n",
    "print(f\"ðŸ“Œ Configuration set:\")\n",
    "print(f\"   Topic: {TOPIC}\")\n",
    "print(f\"   Runs: {RUNS}\")\n",
    "print(f\"   Model: {MODEL}\")\n",
    "print(f\"   Temperature: {TEMPERATURE}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "eab17885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Install Dependencies (run once)\n",
    "# ============================================\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_dependencies():\n",
    "    \"\"\"Install required packages if not already installed.\"\"\"\n",
    "    packages = ['openai>=1.0.0', 'pyyaml', 'python-dotenv']\n",
    "    for package in packages:\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', package])\n",
    "\n",
    "# Uncomment to install\n",
    "# install_dependencies()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "01072554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… OpenAI API key loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Import Libraries and Load Configuration\n",
    "# ============================================\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from typing import List, Dict, Optional\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Check if API key is set\n",
    "if not os.getenv('OPENAI_API_KEY'):\n",
    "    print(\"âš ï¸  Warning: OPENAI_API_KEY not found in environment variables!\")\n",
    "    print(\"   Please create a .env file with: OPENAI_API_KEY=your-key-here\")\n",
    "else:\n",
    "    print(\"âœ… OpenAI API key loaded successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbacf92c",
   "metadata": {},
   "source": [
    "DocumentationGenerator Class is the core API interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "79859762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Define the DocumentationGenerator Class\n",
    "# ============================================\n",
    "class DocumentationGenerator:\n",
    "    def __init__(self, prompt_yaml_path: str = 'prompt.yaml', examples_dir: str = 'examples/'):\n",
    "        \"\"\"Initialize the documentation generator with configuration.\"\"\"\n",
    "        self.client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "        self.examples_dir = Path(examples_dir)\n",
    "        self.prompt_config = self._load_prompt_config(prompt_yaml_path)\n",
    "        self.examples = self._load_examples()\n",
    "        \n",
    "    def _load_prompt_config(self, path: str) -> dict:\n",
    "        \"\"\"Load the prompt configuration from YAML file.\"\"\"\n",
    "        try:\n",
    "            with open(path, 'r') as f:\n",
    "                return yaml.safe_load(f)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: {path} not found. Using default configuration.\")\n",
    "            return {\n",
    "                'system_prompt': 'You are a technical documentation expert.',\n",
    "                'documentation_structure': ['Description', 'Installation', 'Usage', 'Examples', 'References']\n",
    "            }\n",
    "    \n",
    "    def _load_examples(self) -> List[Dict[str, str]]:\n",
    "        \"\"\"Load few-shot examples from YAML files.\"\"\"\n",
    "        examples = []\n",
    "        \n",
    "        # Ensure examples directory exists\n",
    "        self.examples_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Load YAML examples\n",
    "        yaml_files = sorted(self.examples_dir.glob('*.yaml'))\n",
    "        for yaml_file in yaml_files:\n",
    "            try:\n",
    "                with open(yaml_file, 'r') as f:\n",
    "                    msgs = yaml.safe_load(f)\n",
    "                    if isinstance(msgs, list):\n",
    "                        examples.extend(msgs)\n",
    "                    else:\n",
    "                        examples.append(msgs)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {yaml_file}: {e}\")\n",
    "        \n",
    "        # Load HTML examples if needed for reference\n",
    "        html_files = sorted(self.examples_dir.glob('*.html'))\n",
    "        for html_file in html_files:\n",
    "            try:\n",
    "                with open(html_file, 'r') as f:\n",
    "                    content = f.read()\n",
    "                    # Add as assistant example showing the expected format\n",
    "                    examples.append({\n",
    "                        'role': 'assistant',\n",
    "                        'content': content,\n",
    "                        'metadata': {'filename': html_file.name}\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {html_file}: {e}\")\n",
    "        \n",
    "        return examples\n",
    "    \n",
    "    def _extract_topic_from_query(self, query: str) -> str:\n",
    "        \"\"\"Extract the main topic from the query for filename generation.\"\"\"\n",
    "        # Try to extract topic using various patterns\n",
    "        patterns = [\n",
    "            r'documentation for (\\w+)',\n",
    "            r'using (\\w+)',\n",
    "            r'about (\\w+)',\n",
    "            r'for (\\w+) documentation',\n",
    "        ]\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            match = re.search(pattern, query, re.IGNORECASE)\n",
    "            if match:\n",
    "                return match.group(1).lower().replace(' ', '-')\n",
    "        \n",
    "        # Fallback: use first significant word\n",
    "        words = query.split()\n",
    "        for word in words:\n",
    "            if len(word) > 3 and word.lower() not in ['create', 'make', 'generate', 'write']:\n",
    "                return word.lower()\n",
    "        \n",
    "        return 'documentation'\n",
    "    \n",
    "    def _build_system_prompt(self) -> str:\n",
    "        \"\"\"Build the system prompt from configuration.\"\"\"\n",
    "        base_prompt = self.prompt_config.get('system_prompt', \n",
    "            'You are a technical documentation expert creating HTML knowledge base articles.')\n",
    "        \n",
    "        # Add structure information if available\n",
    "        if 'documentation_structure' in self.prompt_config:\n",
    "            structure = self.prompt_config['documentation_structure']\n",
    "            base_prompt += f\"\\n\\nEach article should follow this structure:\\n\"\n",
    "            base_prompt += \"\\n\".join(f\"- {section}\" for section in structure)\n",
    "        \n",
    "        # Add any terms/definitions\n",
    "        if 'terms' in self.prompt_config:\n",
    "            base_prompt += \"\\n\\nKey terms:\\n\"\n",
    "            for term, definition in self.prompt_config['terms'].items():\n",
    "                base_prompt += f\"- {term}: {definition}\\n\"\n",
    "        \n",
    "        return base_prompt\n",
    "    \n",
    "    def generate_documentation(self, query: str, runs: int = 5, \n",
    "                             model: str = 'gpt-4', \n",
    "                             temperature: float = 0.7) -> List[str]:\n",
    "        \"\"\"Generate multiple documentation pages based on the query.\"\"\"\n",
    "        topic = self._extract_topic_from_query(query)\n",
    "        generated_files = []\n",
    "        \n",
    "        # Build messages\n",
    "        system_prompt = self._build_system_prompt()\n",
    "        \n",
    "        for i in range(runs):\n",
    "            try:\n",
    "                messages = [\n",
    "                    {'role': 'system', 'content': system_prompt}\n",
    "                ]\n",
    "                \n",
    "                # Add few-shot examples\n",
    "                for example in self.examples:\n",
    "                    # Only add role and content, skip metadata\n",
    "                    messages.append({\n",
    "                        'role': example['role'],\n",
    "                        'content': example['content']\n",
    "                    })\n",
    "                \n",
    "                # Add the actual query\n",
    "                messages.append({'role': 'user', 'content': query})\n",
    "                \n",
    "                # Make API call\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=model,\n",
    "                    messages=messages,\n",
    "                    temperature=temperature\n",
    "                )\n",
    "                \n",
    "                content = response.choices[0].message.content.strip()\n",
    "                \n",
    "                # Generate filename based on topic, model, temperature, and iteration\n",
    "                # Clean model name (remove special characters)\n",
    "                model_name = model.replace('-', '').replace('.', '')\n",
    "                temp_str = str(temperature).replace('.', '')\n",
    "\n",
    "                if runs == 1:\n",
    "                    filename = f'{topic}_{model_name}_temp{temp_str}.html'\n",
    "                else:\n",
    "                    filename = f'{topic}_{model_name}_temp{temp_str}_v{i+1}.html'                \n",
    "                # Save the response\n",
    "                output_dir = Path('output')\n",
    "                output_dir.mkdir(exist_ok=True)\n",
    "                \n",
    "                filepath = output_dir / filename\n",
    "                with open(filepath, 'w', encoding='utf-8') as f:\n",
    "                    f.write(content)\n",
    "                \n",
    "                generated_files.append(str(filepath))\n",
    "                print(f\"âœ“ Generated: {filepath}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"âœ— Error generating documentation (run {i+1}): {e}\")\n",
    "        \n",
    "        return generated_files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21327314",
   "metadata": {},
   "source": [
    "Examples pulled from the User Docs website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "dbb24f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Initializing documentation generator...\n",
      "âœ… Generator initialized successfully\n",
      "ðŸ“ Found 4 examples\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Initialize Generator\n",
    "# ============================================\n",
    "print(\"ðŸ”§ Initializing documentation generator...\")\n",
    "\n",
    "try:\n",
    "    generator = DocumentationGenerator(\n",
    "        prompt_yaml_path='prompt.yaml',\n",
    "        examples_dir='examples/'\n",
    "    )\n",
    "    print(\"âœ… Generator initialized successfully\")\n",
    "    print(f\"ðŸ“ Found {len(generator.examples)} examples\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error initializing generator: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8c574760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ðŸ“ Generating documentation for: ParallelHDF5\n",
      "ðŸ“‹ Query: Create a knowledge base article with regards to using ParallelHDF5 on the FASRC cluster, using the tone of graduate level Academic Computing documentation.\n",
      "ðŸ”„ Generating 5 variations...\n",
      "============================================================\n",
      "\n",
      "âœ“ Generated: output/parallelhdf5_gpt4omini_temp07_v1.html\n",
      "âœ“ Generated: output/parallelhdf5_gpt4omini_temp07_v2.html\n",
      "âœ“ Generated: output/parallelhdf5_gpt4omini_temp07_v3.html\n",
      "âœ“ Generated: output/parallelhdf5_gpt4omini_temp07_v4.html\n",
      "âœ“ Generated: output/parallelhdf5_gpt4omini_temp07_v5.html\n",
      "\n",
      "============================================================\n",
      "âœ… Generation complete!\n",
      "â±ï¸  Time taken: 101.67 seconds\n",
      "ðŸ“ Generated 5 files:\n",
      "   - output/parallelhdf5_gpt4omini_temp07_v1.html\n",
      "   - output/parallelhdf5_gpt4omini_temp07_v2.html\n",
      "   - output/parallelhdf5_gpt4omini_temp07_v3.html\n",
      "   - output/parallelhdf5_gpt4omini_temp07_v4.html\n",
      "   - output/parallelhdf5_gpt4omini_temp07_v5.html\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Generate Documentation for Single Topic\n",
    "# ============================================\n",
    "# This cell uses the TOPIC variable defined in Cell 1\n",
    "\n",
    "# Build the query from template\n",
    "query = QUERY_TEMPLATE.format(topic=TOPIC)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ðŸ“ Generating documentation for: {TOPIC}\")\n",
    "print(f\"ðŸ“‹ Query: {query}\")\n",
    "print(f\"ðŸ”„ Generating {RUNS} variations...\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Track generation time\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Generate the documentation\n",
    "files = generator.generate_documentation(\n",
    "    query=query,\n",
    "    runs=RUNS,\n",
    "    model=MODEL,\n",
    "    temperature=TEMPERATURE\n",
    ")\n",
    "\n",
    "# Calculate elapsed time\n",
    "elapsed = (datetime.now() - start_time).total_seconds()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"âœ… Generation complete!\")\n",
    "print(f\"â±ï¸  Time taken: {elapsed:.2f} seconds\")\n",
    "print(f\"ðŸ“ Generated {len(files)} files:\")\n",
    "for file in files:\n",
    "    print(f\"   - {file}\")\n",
    "print(f\"{'='*60}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd5f0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Batch Generation for Multiple Topics (Optional)\n",
    "# ============================================\n",
    "# Uncomment and run this cell to generate docs for multiple topics at once\n",
    "\n",
    "# TOPICS_LIST = [\"PyTorch\", \"TensorFlow\", \"OpenMP\", \"MATLAB\", \"Conda\"]\n",
    "# \n",
    "# for topic in TOPICS_LIST:\n",
    "#     query = QUERY_TEMPLATE.format(topic=topic)\n",
    "#     print(f\"\\nðŸ“ Generating documentation for: {topic}\")\n",
    "#     \n",
    "#     files = generator.generate_documentation(\n",
    "#         query=query,\n",
    "#         runs=1,  # Just one version per topic for batch\n",
    "#         model=MODEL,\n",
    "#         temperature=TEMPERATURE\n",
    "#     )\n",
    "#     \n",
    "#     print(f\"âœ… Generated: {', '.join(files)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ffa28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Preview Generated Files (Optional)\n",
    "# ============================================\n",
    "# This cell lets you preview the generated HTML files\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "import os\n",
    "\n",
    "# Get the most recently generated file\n",
    "output_dir = Path('output')\n",
    "if output_dir.exists():\n",
    "    html_files = sorted(output_dir.glob(f'{TOPIC.lower()}*.html'))\n",
    "    if html_files:\n",
    "        latest_file = html_files[-1]\n",
    "        print(f\"ðŸ“„ Previewing: {latest_file.name}\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        with open(latest_file, 'r') as f:\n",
    "            content = f.read()\n",
    "            # Show first 1000 characters\n",
    "            print(content[:1000] + \"...\" if len(content) > 1000 else content)\n",
    "            \n",
    "        # Optionally display as rendered HTML (uncomment if in Jupyter)\n",
    "        # display(HTML(content))\n",
    "    else:\n",
    "        print(f\"No files found for topic: {TOPIC}\")\n",
    "else:\n",
    "    print(\"Output directory not found!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a687a53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Compare Multiple Versions (Optional)\n",
    "# ============================================\n",
    "# This cell helps you compare different generated versions\n",
    "\n",
    "def compare_versions(topic: str):\n",
    "    \"\"\"Compare key differences between generated versions.\"\"\"\n",
    "    output_dir = Path('output')\n",
    "    files = sorted(output_dir.glob(f'{topic.lower()}_*.html'))\n",
    "    \n",
    "    if len(files) < 2:\n",
    "        print(\"Need at least 2 versions to compare\")\n",
    "        return\n",
    "    \n",
    "    print(f\"ðŸ“Š Comparing {len(files)} versions of {topic} documentation:\\n\")\n",
    "    \n",
    "    for i, file in enumerate(files, 1):\n",
    "        with open(file, 'r') as f:\n",
    "            content = f.read()\n",
    "            \n",
    "        # Extract some metrics\n",
    "        word_count = len(content.split())\n",
    "        line_count = len(content.splitlines())\n",
    "        has_examples = 'example' in content.lower()\n",
    "        has_code_blocks = '<code>' in content or '<pre>' in content\n",
    "        \n",
    "        print(f\"Version {i} ({file.name}):\")\n",
    "        print(f\"  - Words: {word_count}\")\n",
    "        print(f\"  - Lines: {line_count}\")\n",
    "        print(f\"  - Has examples: {'Yes' if has_examples else 'No'}\")\n",
    "        print(f\"  - Has code blocks: {'Yes' if has_code_blocks else 'No'}\")\n",
    "        print()\n",
    "\n",
    "# Run comparison\n",
    "compare_versions(TOPIC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414a2d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: HTML Parser and Section Extractor\n",
    "# ============================================\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from typing import Dict, List, Tuple\n",
    "import difflib\n",
    "\n",
    "class DocumentAnalyzer:\n",
    "    \"\"\"Analyze and extract sections from HTML documentation.\"\"\"\n",
    "    \n",
    "    def __init__(self, section_headers: List[str] = None):\n",
    "        self.section_headers = section_headers or [\n",
    "            'Description', 'Installation', 'Usage', 'Examples', 'References'\n",
    "        ]\n",
    "        \n",
    "    def extract_sections(self, html_content: str) -> Dict[str, str]:\n",
    "        \"\"\"Extract sections from HTML content based on headers.\"\"\"\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        sections = {}\n",
    "        \n",
    "        # Find all headers (h1, h2, h3, etc.)\n",
    "        headers = soup.find_all(['h1', 'h2', 'h3', 'h4'])\n",
    "        \n",
    "        for i, header in enumerate(headers):\n",
    "            header_text = header.get_text().strip()\n",
    "            \n",
    "            # Check if this header matches any of our target sections\n",
    "            for section_name in self.section_headers:\n",
    "                if section_name.lower() in header_text.lower():\n",
    "                    # Extract content between this header and the next\n",
    "                    content_parts = []\n",
    "                    \n",
    "                    # Get all siblings until the next header\n",
    "                    for sibling in header.find_next_siblings():\n",
    "                        if sibling.name in ['h1', 'h2', 'h3', 'h4']:\n",
    "                            break\n",
    "                        content_parts.append(str(sibling))\n",
    "                    \n",
    "                    sections[section_name] = '\\n'.join(content_parts)\n",
    "                    break\n",
    "        \n",
    "        return sections\n",
    "    \n",
    "    def calculate_section_score(self, section_content: str, section_name: str) -> float:\n",
    "        \"\"\"Calculate a quality score for a section.\"\"\"\n",
    "        if not section_content:\n",
    "            return 0.0\n",
    "        \n",
    "        soup = BeautifulSoup(section_content, 'html.parser')\n",
    "        text = soup.get_text().strip()\n",
    "        \n",
    "        # Base score on multiple factors\n",
    "        score = 0.0\n",
    "        \n",
    "        # Length (not too short, not too long)\n",
    "        word_count = len(text.split())\n",
    "        if section_name == \"Description\":\n",
    "            ideal_length = 150\n",
    "            score += max(0, 1 - abs(word_count - ideal_length) / ideal_length) * 20\n",
    "        else:\n",
    "            score += min(word_count / 100, 1) * 20  # Longer is generally better for other sections\n",
    "        \n",
    "        # Code examples (for Installation, Usage, Examples)\n",
    "        if section_name in [\"Installation\", \"Usage\", \"Examples\"]:\n",
    "            code_blocks = soup.find_all(['code', 'pre'])\n",
    "            score += min(len(code_blocks) * 10, 30)\n",
    "        \n",
    "        # Lists (good for organization)\n",
    "        lists = soup.find_all(['ul', 'ol'])\n",
    "        score += min(len(lists) * 5, 15)\n",
    "        \n",
    "        # Links (good for References)\n",
    "        if section_name == \"References\":\n",
    "            links = soup.find_all('a')\n",
    "            score += min(len(links) * 10, 30)\n",
    "        else:\n",
    "            links = soup.find_all('a')\n",
    "            score += min(len(links) * 2, 10)\n",
    "        \n",
    "        # Formatting variety (bold, italic, etc.)\n",
    "        formatting_tags = soup.find_all(['strong', 'em', 'b', 'i'])\n",
    "        score += min(len(formatting_tags) * 2, 10)\n",
    "        \n",
    "        # Clarity (sentences not too long)\n",
    "        sentences = text.split('.')\n",
    "        avg_sentence_length = sum(len(s.split()) for s in sentences) / max(len(sentences), 1)\n",
    "        if 10 <= avg_sentence_length <= 25:\n",
    "            score += 15\n",
    "        \n",
    "        return min(score, 100)  # Cap at 100\n",
    "\n",
    "# Initialize the analyzer\n",
    "analyzer = DocumentAnalyzer()\n",
    "print(\"âœ… Document analyzer initialized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a65bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Load and Analyze All Versions ( Arbitrary )\n",
    "# ============================================\n",
    "%pip install pandas \n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "def load_and_analyze_versions(topic: str, model: str, temperature: str, num_versions: int = 5):\n",
    "    \"\"\"Load all versions and extract their sections.\"\"\"\n",
    "    \n",
    "    output_dir = Path('output')\n",
    "    all_sections = {}\n",
    "    \n",
    "    for version in range(1, num_versions + 1):\n",
    "        # Construct filename\n",
    "        if num_versions == 1:\n",
    "            filename = f'{topic}_{model}_temp{temperature}.html'\n",
    "        else:\n",
    "            filename = f'{topic}_{model}_temp{temperature}_v{version}.html'\n",
    "        \n",
    "        filepath = output_dir / filename\n",
    "        \n",
    "        if not filepath.exists():\n",
    "            print(f\"âš ï¸  File not found: {filepath}\")\n",
    "            continue\n",
    "            \n",
    "        # Load and extract sections\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        sections = analyzer.extract_sections(content)\n",
    "        all_sections[f'Version {version}'] = sections\n",
    "        \n",
    "        print(f\"âœ… Loaded Version {version}: {filename}\")\n",
    "        print(f\"   Found sections: {', '.join(sections.keys())}\")\n",
    "    \n",
    "    return all_sections\n",
    "\n",
    "# Load all versions\n",
    "all_sections = load_and_analyze_versions(\n",
    "    topic=TOPIC.lower(),\n",
    "    model=MODEL.replace('-', '').replace('.', ''),\n",
    "    temperature=str(TEMPERATURE).replace('.', ''),\n",
    "    num_versions=RUNS\n",
    ")\n",
    "\n",
    "print(f\"\\nðŸ“Š Loaded {len(all_sections)} versions for analysis\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c6e9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Score and Compare Sections ( Arbitrary )\n",
    "# ============================================\n",
    "import pandas as pd\n",
    "\n",
    "def analyze_sections(all_sections: Dict[str, Dict[str, str]]) -> pd.DataFrame:\n",
    "    \"\"\"Analyze and score all sections across versions.\"\"\"\n",
    "    \n",
    "    scores_data = []\n",
    "    \n",
    "    for section_name in analyzer.section_headers:\n",
    "        section_scores = {}\n",
    "        \n",
    "        for version, sections in all_sections.items():\n",
    "            if section_name in sections:\n",
    "                score = analyzer.calculate_section_score(\n",
    "                    sections[section_name], \n",
    "                    section_name\n",
    "                )\n",
    "                section_scores[version] = score\n",
    "            else:\n",
    "                section_scores[version] = 0\n",
    "        \n",
    "        # Find best version for this section\n",
    "        if section_scores:\n",
    "            best_version = max(section_scores, key=section_scores.get)\n",
    "            best_score = section_scores[best_version]\n",
    "        else:\n",
    "            best_version = \"N/A\"\n",
    "            best_score = 0\n",
    "        \n",
    "        scores_data.append({\n",
    "            'Section': section_name,\n",
    "            **section_scores,\n",
    "            'Best Version': best_version,\n",
    "            'Best Score': best_score\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(scores_data)\n",
    "\n",
    "# Analyze all sections\n",
    "scores_df = analyze_sections(all_sections)\n",
    "\n",
    "print(\"ðŸ“Š Section Analysis Results:\")\n",
    "print(\"=\"*80)\n",
    "print(scores_df.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Show summary of best versions\n",
    "print(\"\\nðŸ† Best Version for Each Section:\")\n",
    "for _, row in scores_df.iterrows():\n",
    "    print(f\"   {row['Section']}: {row['Best Version']} (Score: {row['Best Score']:.1f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "854acbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Compile Best-of Document\n",
    "# ============================================\n",
    "def compile_best_document(all_sections: Dict[str, Dict[str, str]], \n",
    "                         scores_data: pd.DataFrame,\n",
    "                         manual_overrides: Dict[str, str] = None) -> str:\n",
    "    \"\"\"Compile the best sections into a final document.\"\"\"\n",
    "    \n",
    "    # Allow manual overrides if needed\n",
    "    manual_overrides = manual_overrides or {}\n",
    "    \n",
    "    # Start building the final HTML\n",
    "    html_parts = [\n",
    "        '<!DOCTYPE html>',\n",
    "        '<html lang=\"en\">',\n",
    "        '<head>',\n",
    "        '<meta charset=\"UTF-8\">',\n",
    "        '<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">',\n",
    "        f'<title>{TOPIC} Documentation - Best Compilation</title>',\n",
    "        '<style>',\n",
    "        'body { font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto, sans-serif; ',\n",
    "        '       line-height: 1.6; max-width: 900px; margin: 0 auto; padding: 20px; }',\n",
    "        'h1, h2, h3 { color: #2c3e50; }',\n",
    "        'code { background-color: #f4f4f4; padding: 2px 4px; border-radius: 3px; }',\n",
    "        'pre { background-color: #f4f4f4; padding: 15px; border-radius: 5px; overflow-x: auto; }',\n",
    "        '.metadata { background-color: #e8f4f8; padding: 15px; border-radius: 5px; ',\n",
    "        '            margin-bottom: 30px; font-size: 0.9em; }',\n",
    "        '.section { margin-bottom: 40px; }',\n",
    "        '.version-note { color: #7f8c8d; font-size: 0.85em; font-style: italic; }',\n",
    "        '</style>',\n",
    "        '</head>',\n",
    "        '<body>',\n",
    "        f'<h1>{TOPIC} Documentation</h1>',\n",
    "        '<div class=\"metadata\">',\n",
    "        f'<strong>Compiled from best sections across {len(all_sections)} versions</strong><br>',\n",
    "        f'Generated using: {MODEL} (Temperature: {TEMPERATURE})<br>',\n",
    "        f'Compilation date: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}',\n",
    "        '</div>'\n",
    "    ]\n",
    "    \n",
    "    # Add each section\n",
    "    for _, row in scores_df.iterrows():\n",
    "        section_name = row['Section']\n",
    "        best_version = manual_overrides.get(section_name, row['Best Version'])\n",
    "        \n",
    "        if best_version != \"N/A\" and best_version in all_sections:\n",
    "            if section_name in all_sections[best_version]:\n",
    "                html_parts.append(f'<div class=\"section\">')\n",
    "                html_parts.append(f'<h2>{section_name}</h2>')\n",
    "                html_parts.append(f'<span class=\"version-note\">From {best_version}</span>')\n",
    "                html_parts.append(all_sections[best_version][section_name])\n",
    "                html_parts.append('</div>')\n",
    "    \n",
    "    # Close HTML\n",
    "    html_parts.extend([\n",
    "        '<div class=\"metadata\" style=\"margin-top: 50px;\">',\n",
    "        '<strong>Section Sources:</strong><br>',\n",
    "    ])\n",
    "    \n",
    "    # Add section source summary\n",
    "    for _, row in scores_df.iterrows():\n",
    "        section_name = row['Section']\n",
    "        best_version = manual_overrides.get(section_name, row['Best Version'])\n",
    "        score = row['Best Score']\n",
    "        html_parts.append(f'{section_name}: {best_version} (Score: {score:.1f})<br>')\n",
    "    \n",
    "    html_parts.extend([\n",
    "        '</div>',\n",
    "        '</body>',\n",
    "        '</html>'\n",
    "    ])\n",
    "    \n",
    "    return '\\n'.join(html_parts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782fd9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: Manual Override Option (if needed)\n",
    "# ============================================\n",
    "# If you disagree with the automatic selection, you can manually override\n",
    "\n",
    "# Example: Force specific versions for certain sections\n",
    "# manual_overrides = {\n",
    "#     \"Description\": \"Version 2\",  # Use Version 2's description instead\n",
    "#     \"Examples\": \"Version 4\"      # Use Version 4's examples instead\n",
    "# }\n",
    "# \n",
    "# # Recompile with overrides\n",
    "# best_document_html = compile_best_document(all_sections, scores_data, manual_overrides)\n",
    "# output_path = Path('output') / f'{TOPIC.lower()}_best_compilation_manual.html'\n",
    "# with open(output_path, 'w', encoding='utf-8') as f:\n",
    "#     f.write(best_document_html)\n",
    "# print(f\"âœ… Manual compilation saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e2a6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16: Generate Comparison Report ( Arbitrary )\n",
    "# ============================================\n",
    "def generate_analysis_report():\n",
    "    \"\"\"Generate a detailed analysis report of the compilation process.\"\"\"\n",
    "    \n",
    "    report = []\n",
    "    report.append(f\"# Documentation Analysis Report for {TOPIC}\")\n",
    "    report.append(f\"\\nGenerated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    report.append(f\"Model: {MODEL}, Temperature: {TEMPERATURE}\")\n",
    "    report.append(f\"Analyzed {len(all_sections)} versions\\n\")\n",
    "    \n",
    "    report.append(\"## Section Scores\\n\")\n",
    "    report.append(\"| Section | \" + \" | \".join(all_sections.keys()) + \" | Best Version | Best Score |\")\n",
    "    report.append(\"|---------|\" + \"---|\" * (len(all_sections) + 2))\n",
    "    \n",
    "    for row in scores_data:\n",
    "        line = f\"| {row['Section']} | \"\n",
    "        for version in all_sections.keys():\n",
    "            score = row.get(version, 0)\n",
    "            line += f\"{score:.1f} | \"\n",
    "        line += f\"{row['Best Version']} | {row['Best Score']:.1f} |\"\n",
    "        report.append(line)\n",
    "    \n",
    "    report.append(\"\\n## Key Findings\\n\")\n",
    "    \n",
    "    # Find most consistent version\n",
    "    version_totals = {}\n",
    "    for version in all_sections.keys():\n",
    "        total = sum(row.get(version, 0) for row in scores_data)\n",
    "        version_totals[version] = total\n",
    "    \n",
    "    if version_totals:\n",
    "        best_overall = max(version_totals, key=version_totals.get)\n",
    "        report.append(f\"- **Best Overall Version**: {best_overall} (Total Score: {version_totals[best_overall]:.1f})\")\n",
    "    \n",
    "    # Find sections with high variance\n",
    "    report.append(\"\\n### Section Quality Variance\")\n",
    "    for row in scores_data:\n",
    "        version_scores = [row.get(v, 0) for v in all_sections.keys()]\n",
    "        if version_scores:\n",
    "            variance = max(version_scores) - min(version_scores)\n",
    "            if variance > 20:\n",
    "                report.append(f\"- **{row['Section']}**: High variance ({variance:.1f} points) - quality varies significantly between versions\")\n",
    "    \n",
    "    report.append(\"\\n## Recommendations\")\n",
    "    report.append(\"- Review sections with high variance manually\")\n",
    "    report.append(\"- Consider regenerating sections with scores below 50\")\n",
    "    report.append(f\"- The compiled document uses the best scoring section from each category\")\n",
    "    \n",
    "    return '\\n'.join(report)\n",
    "\n",
    "# Generate and save the report\n",
    "report_content = generate_analysis_report()\n",
    "report_path = Path('output') / f'{TOPIC.lower()}_analysis_report.md'\n",
    "with open(report_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(report_content)\n",
    "\n",
    "print(f\"ðŸ“„ Analysis report saved to: {report_path}\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"âœ… Best compilation: output/{TOPIC.lower()}_best_compilation.html\")\n",
    "print(f\"ðŸ“Š Analysis report: {report_path}\")\n",
    "print(f\"ðŸ“ˆ View comparison: Open comparison.html in browser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5e81cf34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… GPT Quality Evaluator initialized\n"
     ]
    }
   ],
   "source": [
    "# Cell 17: GPT-Based Quality Evaluation\n",
    "# ============================================\n",
    "import json\n",
    "from typing import Dict, List, Tuple\n",
    "import asyncio\n",
    "import time\n",
    "\n",
    "class GPTQualityEvaluator:\n",
    "    \"\"\"Evaluate documentation quality using GPT for subjective metrics.\"\"\"\n",
    "    \n",
    "    def __init__(self, client, model='gpt-4'):\n",
    "        self.client = client\n",
    "        self.model = model\n",
    "        \n",
    "    def create_evaluation_prompt(self, section_content: str, section_name: str, \n",
    "                               topic: str, criteria: str) -> str:\n",
    "        \"\"\"Create a prompt for evaluating specific quality criteria.\"\"\"\n",
    "        \n",
    "        prompts = {\n",
    "            \"technical_accuracy\": f\"\"\"\n",
    "Evaluate the technical accuracy of this {section_name} section about {topic}.\n",
    "Consider:\n",
    "- Are the commands, code examples, and technical details correct?\n",
    "- Are version numbers, dependencies, and requirements accurate?\n",
    "- Are there any outdated or incorrect technical statements?\n",
    "- Would following these instructions actually work?\n",
    "\n",
    "Section content:\n",
    "{section_content}\n",
    "\n",
    "Provide a score from 0-100 and a brief explanation (2-3 sentences).\n",
    "Format: {{\"score\": NUMBER, \"explanation\": \"...\"}}\n",
    "\"\"\",\n",
    "            \n",
    "            \"writing_style\": f\"\"\"\n",
    "Evaluate the writing style and tone of this {section_name} section for academic/research computing documentation.\n",
    "Consider:\n",
    "- Is the tone appropriately professional and academic?\n",
    "- Is it clear and accessible for graduate-level users?\n",
    "- Does it avoid being too casual or too dense?\n",
    "- Is the language consistent and well-structured?\n",
    "\n",
    "Section content:\n",
    "{section_content}\n",
    "\n",
    "Provide a score from 0-100 and a brief explanation (2-3 sentences).\n",
    "Format: {{\"score\": NUMBER, \"explanation\": \"...\"}}\n",
    "\"\"\",\n",
    "            \n",
    "            \"completeness\": f\"\"\"\n",
    "Evaluate the completeness of this {section_name} section about {topic}.\n",
    "Consider:\n",
    "- Does it cover all essential information for this section type?\n",
    "- Are there important details or steps missing?\n",
    "- For {section_name}, what key elements should be present?\n",
    "- Does it answer the questions users would typically have?\n",
    "\n",
    "Section content:\n",
    "{section_content}\n",
    "\n",
    "Provide a score from 0-100 and a brief explanation (2-3 sentences).\n",
    "Format: {{\"score\": NUMBER, \"explanation\": \"...\"}}\n",
    "\"\"\"\n",
    "        }\n",
    "        \n",
    "        return prompts.get(criteria, \"\")\n",
    "    \n",
    "    def parse_gpt_response(self, response: str) -> Tuple[float, str]:\n",
    "        \"\"\"Parse the GPT response to extract score and explanation.\"\"\"\n",
    "        try:\n",
    "            # Try to parse as JSON first\n",
    "            result = json.loads(response)\n",
    "            return result['score'], result['explanation']\n",
    "        except:\n",
    "            # Fallback: extract number and text\n",
    "            import re\n",
    "            score_match = re.search(r'\\b(\\d+)\\b', response)\n",
    "            score = float(score_match.group(1)) if score_match else 50.0\n",
    "            \n",
    "            # Extract explanation (everything after the score)\n",
    "            explanation = response.split(str(int(score)), 1)[-1].strip()\n",
    "            return score, explanation\n",
    "    \n",
    "    async def evaluate_section_async(self, section_content: str, section_name: str, \n",
    "                                   topic: str, criteria: str) -> Dict:\n",
    "        \"\"\"Evaluate a single section on a single criterion asynchronously.\"\"\"\n",
    "        if not section_content.strip():\n",
    "            return {\n",
    "                'criteria': criteria,\n",
    "                'score': 0,\n",
    "                'explanation': 'Section is empty'\n",
    "            }\n",
    "        \n",
    "        # Truncate very long sections to stay within token limits\n",
    "        max_chars = 3000\n",
    "        if len(section_content) > max_chars:\n",
    "            section_content = section_content[:max_chars] + \"... [truncated]\"\n",
    "        \n",
    "        prompt = self.create_evaluation_prompt(section_content, section_name, topic, criteria)\n",
    "        \n",
    "        try:\n",
    "            response = await asyncio.to_thread(\n",
    "                self.client.chat.completions.create,\n",
    "                model=self.model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are an expert technical documentation reviewer.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0.3,  # Lower temperature for more consistent evaluation\n",
    "                max_tokens=150\n",
    "            )\n",
    "            \n",
    "            score, explanation = self.parse_gpt_response(response.choices[0].message.content)\n",
    "            \n",
    "            return {\n",
    "                'criteria': criteria,\n",
    "                'score': score,\n",
    "                'explanation': explanation\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'criteria': criteria,\n",
    "                'score': 0,\n",
    "                'explanation': f'Error during evaluation: {str(e)}'\n",
    "            }\n",
    "    \n",
    "    def evaluate_section(self, section_content: str, section_name: str, \n",
    "                        topic: str, criteria: List[str] = None) -> Dict[str, Dict]:\n",
    "        \"\"\"Evaluate a section on multiple criteria.\"\"\"\n",
    "        if criteria is None:\n",
    "            criteria = ['technical_accuracy', 'writing_style', 'completeness']\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        for criterion in criteria:\n",
    "            if not section_content.strip():\n",
    "                results[criterion] = {\n",
    "                    'score': 0,\n",
    "                    'explanation': 'Section is empty'\n",
    "                }\n",
    "                continue\n",
    "            \n",
    "            # Truncate very long sections\n",
    "            max_chars = 3000\n",
    "            content = section_content[:max_chars] + \"...\" if len(section_content) > max_chars else section_content\n",
    "            \n",
    "            prompt = self.create_evaluation_prompt(content, section_name, topic, criterion)\n",
    "            \n",
    "            try:\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=self.model,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": \"You are an expert technical documentation reviewer.\"},\n",
    "                        {\"role\": \"user\", \"content\": prompt}\n",
    "                    ],\n",
    "                    temperature=0.3,\n",
    "                    max_tokens=150\n",
    "                )\n",
    "                \n",
    "                score, explanation = self.parse_gpt_response(response.choices[0].message.content)\n",
    "                results[criterion] = {\n",
    "                    'score': score,\n",
    "                    'explanation': explanation\n",
    "                }\n",
    "                \n",
    "            except Exception as e:\n",
    "                results[criterion] = {\n",
    "                    'score': 0,\n",
    "                    'explanation': f'Error: {str(e)}'\n",
    "                }\n",
    "            \n",
    "            # Rate limiting\n",
    "            time.sleep(0.5)\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Initialize the GPT evaluator\n",
    "gpt_evaluator = GPTQualityEvaluator(generator.client, model=MODEL)\n",
    "print(\"âœ… GPT Quality Evaluator initialized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a978d231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ¤– Starting GPT evaluation of 9 section-version pairs...\n",
      "   Sections: Description, Installation, Examples\n",
      "   Versions: Version 3, Version 5, Version 4\n",
      "\n",
      "   [1/9] Evaluating Description - Version 3... âœ“ (Composite: 83.3)\n",
      "   [2/9] Evaluating Description - Version 5... âœ“ (Composite: 80.0)\n",
      "   [3/9] Evaluating Description - Version 4... âœ“ (Composite: 83.3)\n",
      "   [4/9] Evaluating Installation - Version 3... âœ“ (Composite: 80.0)\n",
      "   [5/9] Evaluating Installation - Version 5... âœ“ (Composite: 80.0)\n",
      "   [6/9] Evaluating Installation - Version 4... âœ“ (Composite: 80.0)\n",
      "   [7/9] Evaluating Examples - Version 3... âœ“ (Composite: 61.7)\n",
      "   [8/9] Evaluating Examples - Version 5... âœ“ (Composite: 63.3)\n",
      "   [9/9] Evaluating Examples - Version 4... âœ“ (Composite: 71.7)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Cell 18: Evaluate All Versions with GPT\n",
    "# ============================================\n",
    "def evaluate_all_versions_with_gpt(all_sections: Dict, topic: str, \n",
    "                                  sections_to_evaluate: List[str] = None,\n",
    "                                  sample_versions: int = None) -> Dict:\n",
    "    \"\"\"Evaluate all versions using GPT for quality metrics.\"\"\"\n",
    "    \n",
    "    if sections_to_evaluate is None:\n",
    "        sections_to_evaluate = ['Description', 'Installation', 'Usage']\n",
    "    \n",
    "    # Option to sample fewer versions to reduce API calls\n",
    "    versions_to_eval = list(all_sections.keys())\n",
    "    if sample_versions and sample_versions < len(versions_to_eval):\n",
    "        import random\n",
    "        versions_to_eval = random.sample(versions_to_eval, sample_versions)\n",
    "    \n",
    "    gpt_scores = {}\n",
    "    total_evaluations = len(sections_to_evaluate) * len(versions_to_eval)\n",
    "    current_eval = 0\n",
    "    \n",
    "    print(f\"\\nðŸ¤– Starting GPT evaluation of {total_evaluations} section-version pairs...\")\n",
    "    print(f\"   Sections: {', '.join(sections_to_evaluate)}\")\n",
    "    print(f\"   Versions: {', '.join(versions_to_eval)}\\n\")\n",
    "    \n",
    "    for section_name in sections_to_evaluate:\n",
    "        gpt_scores[section_name] = {}\n",
    "        \n",
    "        for version in versions_to_eval:\n",
    "            current_eval += 1\n",
    "            print(f\"   [{current_eval}/{total_evaluations}] Evaluating {section_name} - {version}...\", end='', flush=True)\n",
    "            \n",
    "            if section_name in all_sections[version]:\n",
    "                section_html = all_sections[version][section_name]\n",
    "                # Convert HTML to text for evaluation\n",
    "                soup = BeautifulSoup(section_html, 'html.parser')\n",
    "                section_text = soup.get_text().strip()\n",
    "                \n",
    "                # Evaluate with GPT\n",
    "                results = gpt_evaluator.evaluate_section(\n",
    "                    section_text, \n",
    "                    section_name,\n",
    "                    topic,\n",
    "                    ['technical_accuracy', 'writing_style', 'completeness']\n",
    "                )\n",
    "                \n",
    "                gpt_scores[section_name][version] = results\n",
    "                \n",
    "                # Calculate composite score\n",
    "                composite = sum(r['score'] for r in results.values()) / len(results)\n",
    "                print(f\" âœ“ (Composite: {composite:.1f})\")\n",
    "            else:\n",
    "                gpt_scores[section_name][version] = {\n",
    "                    'technical_accuracy': {'score': 0, 'explanation': 'Section not found'},\n",
    "                    'writing_style': {'score': 0, 'explanation': 'Section not found'},\n",
    "                    'completeness': {'score': 0, 'explanation': 'Section not found'}\n",
    "                }\n",
    "                print(\" âœ— (Not found)\")\n",
    "    \n",
    "    return gpt_scores\n",
    "\n",
    "# Run GPT evaluation (with sampling to reduce costs)\n",
    "gpt_evaluation_results = evaluate_all_versions_with_gpt(\n",
    "    all_sections, \n",
    "    topic=TOPIC,\n",
    "    sections_to_evaluate=['Description', 'Installation', 'Usage', 'Examples'],  # Limit sections\n",
    "    sample_versions=5  # Only evaluate 3 versions instead of all 5 to save API calls\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e2a8ad66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Combined Scoring Results:\n",
      "================================================================================\n",
      "     Section Best Version (Algorithm)  Algorithm Score Best Version (GPT)  GPT Composite  Combined Score Final Best Version\n",
      " Description                Version 4             29.0          Version 3      83.333333           61.60          Version 4\n",
      "Installation                Version 5             58.2          Version 3      80.000000           71.28          Version 5\n",
      "       Usage                Version 4             80.0                N/A       0.000000           32.00          Version 4\n",
      "    Examples                Version 4             52.8          Version 4      71.666667           64.12          Version 4\n",
      "  References                Version 2             54.8                N/A       0.000000           21.92          Version 2\n",
      "================================================================================\n",
      "\n",
      "ðŸ† Final Best Versions (Combined Scoring):\n",
      "   Description: Version 4 (Score: 61.6)\n",
      "   Installation: Version 5 (Score: 71.3)\n",
      "   Usage: Version 4 (Score: 32.0)\n",
      "   Examples: Version 4 (Score: 64.1)\n",
      "   References: Version 2 (Score: 21.9)\n"
     ]
    }
   ],
   "source": [
    "# Cell 19: Combine GPT and Algorithmic Scores\n",
    "# ============================================\n",
    "def create_combined_scoring_report(algorithmic_scores: pd.DataFrame, \n",
    "                                  gpt_scores: Dict,\n",
    "                                  weight_algorithmic: float = 0.4,\n",
    "                                  weight_gpt: float = 0.6) -> pd.DataFrame:\n",
    "    \"\"\"Combine algorithmic and GPT scores with weighting.\"\"\"\n",
    "    \n",
    "    combined_data = []\n",
    "    \n",
    "    for _, row in algorithmic_scores.iterrows():\n",
    "        section_name = row['Section']\n",
    "        \n",
    "        if section_name not in gpt_scores:\n",
    "            # Use only algorithmic score if no GPT evaluation\n",
    "            combined_data.append({\n",
    "                'Section': section_name,\n",
    "                'Best Version (Algorithm)': row['Best Version'],\n",
    "                'Algorithm Score': row['Best Score'],\n",
    "                'Best Version (GPT)': 'N/A',\n",
    "                'GPT Composite': 0,\n",
    "                'Combined Score': row['Best Score'] * weight_algorithmic,\n",
    "                'Final Best Version': row['Best Version']\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        # Find best version according to GPT\n",
    "        gpt_version_scores = {}\n",
    "        for version, results in gpt_scores[section_name].items():\n",
    "            composite = sum(r['score'] for r in results.values()) / len(results)\n",
    "            gpt_version_scores[version] = composite\n",
    "        \n",
    "        best_gpt_version = max(gpt_version_scores, key=gpt_version_scores.get) if gpt_version_scores else 'N/A'\n",
    "        best_gpt_score = gpt_version_scores.get(best_gpt_version, 0)\n",
    "        \n",
    "        # Calculate combined scores for each version\n",
    "        version_combined_scores = {}\n",
    "        for version in all_sections.keys():\n",
    "            algo_score = row.get(version, 0)\n",
    "            gpt_score = gpt_version_scores.get(version, 0)\n",
    "            combined = (algo_score * weight_algorithmic) + (gpt_score * weight_gpt)\n",
    "            version_combined_scores[version] = combined\n",
    "        \n",
    "        # Find best version by combined score\n",
    "        best_combined_version = max(version_combined_scores, key=version_combined_scores.get)\n",
    "        best_combined_score = version_combined_scores[best_combined_version]\n",
    "        \n",
    "        combined_data.append({\n",
    "            'Section': section_name,\n",
    "            'Best Version (Algorithm)': row['Best Version'],\n",
    "            'Algorithm Score': row['Best Score'],\n",
    "            'Best Version (GPT)': best_gpt_version,\n",
    "            'GPT Composite': best_gpt_score,\n",
    "            'Combined Score': best_combined_score,\n",
    "            'Final Best Version': best_combined_version\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(combined_data)\n",
    "\n",
    "# Create combined report\n",
    "combined_scores_df = create_combined_scoring_report(scores_df, gpt_evaluation_results)\n",
    "\n",
    "print(\"\\nðŸ“Š Combined Scoring Results:\")\n",
    "print(\"=\"*80)\n",
    "print(combined_scores_df.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nðŸ† Final Best Versions (Combined Scoring):\")\n",
    "for _, row in combined_scores_df.iterrows():\n",
    "    print(f\"   {row['Section']}: {row['Final Best Version']} (Score: {row['Combined Score']:.1f})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "469df6fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tabulate\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Installing collected packages: tabulate\n",
      "Successfully installed tabulate-0.9.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "ðŸ“„ GPT evaluation report saved to: output/parallelhdf5_gpt_evaluation_report.md\n"
     ]
    }
   ],
   "source": [
    "%pip install tabulate\n",
    "# Cell 20: Generate Detailed GPT Evaluation Report\n",
    "# ============================================\n",
    "def generate_gpt_evaluation_report(gpt_scores: Dict, combined_scores_df: pd.DataFrame) -> str:\n",
    "    \"\"\"Generate a detailed report of GPT evaluations.\"\"\"\n",
    "    \n",
    "    report = []\n",
    "    report.append(f\"# GPT Quality Evaluation Report for {TOPIC}\")\n",
    "    report.append(f\"\\nGenerated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    report.append(f\"Model used for evaluation: {MODEL}\\n\")\n",
    "    \n",
    "    # Summary table\n",
    "    report.append(\"## Evaluation Summary\\n\")\n",
    "    report.append(combined_scores_df.to_markdown(index=False))\n",
    "    \n",
    "    # Detailed evaluations by section\n",
    "    report.append(\"\\n## Detailed GPT Evaluations\\n\")\n",
    "    \n",
    "    for section_name, version_results in gpt_scores.items():\n",
    "        report.append(f\"\\n### {section_name}\\n\")\n",
    "        \n",
    "        for version, criteria_results in version_results.items():\n",
    "            report.append(f\"\\n**{version}:**\\n\")\n",
    "            \n",
    "            for criteria, result in criteria_results.items():\n",
    "                report.append(f\"- **{criteria.replace('_', ' ').title()}**: {result['score']:.1f}/100\")\n",
    "                report.append(f\"  - {result['explanation']}\")\n",
    "            \n",
    "            composite = sum(r['score'] for r in criteria_results.values()) / len(criteria_results)\n",
    "            report.append(f\"- **Composite Score**: {composite:.1f}/100\\n\")\n",
    "    \n",
    "    # Key insights\n",
    "    report.append(\"\\n## Key Insights\\n\")\n",
    "    \n",
    "    # Find sections where GPT and algorithm disagree\n",
    "    disagreements = []\n",
    "    for _, row in combined_scores_df.iterrows():\n",
    "        if row['Best Version (Algorithm)'] != row['Final Best Version']:\n",
    "            disagreements.append(f\"- **{row['Section']}**: Algorithm chose {row['Best Version (Algorithm)']} \"\n",
    "                               f\"but combined scoring chose {row['Final Best Version']}\")\n",
    "    \n",
    "    if disagreements:\n",
    "        report.append(\"### Algorithm vs GPT Disagreements\\n\")\n",
    "        report.extend(disagreements)\n",
    "    \n",
    "    # Technical accuracy concerns\n",
    "    report.append(\"\\n### Technical Accuracy Concerns\\n\")\n",
    "    for section_name, version_results in gpt_scores.items():\n",
    "        for version, criteria_results in version_results.items():\n",
    "            if criteria_results['technical_accuracy']['score'] < 70:\n",
    "                report.append(f\"- **{section_name} ({version})**: \"\n",
    "                            f\"{criteria_results['technical_accuracy']['explanation']}\")\n",
    "    \n",
    "    return '\\n'.join(report)\n",
    "\n",
    "# Generate and save GPT evaluation report\n",
    "gpt_report = generate_gpt_evaluation_report(gpt_evaluation_results, combined_scores_df)\n",
    "gpt_report_path = Path('output') / f'{TOPIC.lower()}_gpt_evaluation_report.md'\n",
    "with open(gpt_report_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(gpt_report)\n",
    "\n",
    "print(f\"\\nðŸ“„ GPT evaluation report saved to: {gpt_report_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f97d85b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… GPT-enhanced compilation saved to: output/parallelhdf5_best_compilation_gpt_enhanced.html\n",
      "\n",
      "================================================================================\n",
      "EVALUATION COMPLETE\n",
      "================================================================================\n",
      "ðŸ“Š Algorithm + GPT best compilation: output/parallelhdf5_best_compilation_gpt_enhanced.html\n",
      "ðŸ“ˆ GPT evaluation report: output/parallelhdf5_gpt_evaluation_report.md\n"
     ]
    }
   ],
   "source": [
    "# Cell 21: Create Final Best Document with GPT Insights\n",
    "# ============================================\n",
    "# Use the combined scoring to create the final document\n",
    "final_overrides = {\n",
    "    row['Section']: row['Final Best Version'] \n",
    "    for _, row in combined_scores_df.iterrows()\n",
    "}\n",
    "\n",
    "# Compile with GPT-informed selections\n",
    "final_best_html = compile_best_document(all_sections, scores_df, final_overrides)\n",
    "\n",
    "# Save the GPT-enhanced compilation\n",
    "final_output_path = Path('output') / f'{TOPIC.lower()}_best_compilation_gpt_enhanced.html'\n",
    "with open(final_output_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(final_best_html)\n",
    "\n",
    "print(f\"âœ… GPT-enhanced compilation saved to: {final_output_path}\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EVALUATION COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"ðŸ“Š Algorithm + GPT best compilation: {final_output_path}\")\n",
    "print(f\"ðŸ“ˆ GPT evaluation report: {gpt_report_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "21c73572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 22: Cost Estimation Helper\n",
    "# ============================================\n",
    "def estimate_api_costs(num_sections: int, num_versions: int, \n",
    "                      avg_section_length: int = 500,\n",
    "                      model: str = 'gpt-4',\n",
    "                      criteria_per_section: int = 3):\n",
    "    \"\"\"Estimate API costs for full evaluation.\"\"\"\n",
    "    \n",
    "    # Approximate tokens\n",
    "    tokens_per_eval = avg_section_length + 200  # content + prompt\n",
    "    total_evals = num_sections * num_versions * criteria_per_section\n",
    "    total_tokens = total_evals * tokens_per_eval\n",
    "    \n",
    "    # Pricing (as of 2024 - update as needed)\n",
    "    prices = {\n",
    "        'gpt-4': {'input': 0.03, 'output': 0.06},  # per 1K tokens\n",
    "        'gpt-3.5-turbo': {'input': 0.0005, 'output': 0.0015}\n",
    "    }\n",
    "    \n",
    "    if model in prices:\n",
    "        # Assume 80% input, 20% output\n",
    "        input_cost = (total_tokens * 0.8 / 1000) * prices[model]['input']\n",
    "        output_cost = (total_tokens * 0.2 / 1000) * prices[model]['output']\n",
    "        total_cost = input_cost + output_cost\n",
    "        \n",
    "        print(f\"\\nðŸ’° Estimated API Costs:\")\n",
    "        print(f\"   Model: {model}\")\n",
    "        print(f\"   Total evaluations: {total_evals}\")\n",
    "        print(f\"   Estimated tokens: {total_tokens:,}\")\n",
    "        print(f\"   Estimated cost: ${total_cost:.2f}\")\n",
    "        print(f\"\\n   Tip: Use gpt-3.5-turbo to reduce costs by ~95%\")\n",
    "    \n",
    "# Estimate costs for full evaluation\n",
    "estimate_api_costs(\n",
    "    num_sections=len(analyzer.section_headers),\n",
    "    num_versions=len(all_sections),\n",
    "    model=MODEL\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatgpt-fewshot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
