<html>
<head>
    <title>Parallel Computing on FASRC Cluster</title>
</head>
<body>
    <article>
        <h1>Parallel Computing on FASRC Cluster</h1>
        
        <h2>Description</h2>
        <p>Parallel computing is a type of computation in which multiple calculations or processes are carried out simultaneously. This approach is essential in high-performance computing (HPC) environments, such as the FASRC (Faculty Arts and Sciences Research Computing) cluster, where tasks are distributed across multiple nodes to significantly reduce execution time. By leveraging the capabilities of multiple processors, parallel computing enables researchers to solve complex problems and process large datasets efficiently.</p>
        
        <h2>Installation</h2>
        <p>To utilize parallel computing capabilities on the FASRC cluster, you must first load the appropriate MPI (Message Passing Interface) module. The available MPI implementations are OpenMPI and MPICH. You can choose between these based on your specific requirements:</p>
        <ul>
            <li>For OpenMPI, use the following command:</li>
            <pre><code>module load openmpi/4.1.4-fasrc01</code></pre>
            <li>For MPICH, use the command:</li>
            <pre><code>module load mpich/4.3.0-fasrc01</code></pre>
        </ul>
        <p>Ensure that you have the necessary permissions and that your account is set up for using these modules. You can check available modules using <code>module avail</code>.</p>
        
        <h2>Usage</h2>
        <p>Once the desired MPI module is loaded, you can write and execute parallel programs using MPI. The typical workflow involves:</p>
        <ol>
            <li>Writing your parallel code in C, C++, or Fortran, utilizing MPI functions for inter-process communication.</li>
            <li>Compiling your code with the appropriate MPI compiler wrapper, for example:</li>
            <pre><code>mpicc my_program.c -o my_program</code></pre>
            <li>Submitting your job to the SLURM scheduler. A sample SLURM job script might look like this:</li>
            <pre><code>#!/bin/bash
#SBATCH --job-name=my_parallel_job
#SBATCH --ntasks=8
#SBATCH --time=01:00:00
#SBATCH --partition=shared

module load openmpi/4.1.4-fasrc01
srun ./my_program</code></pre>
            <li>Submit your job using:</li>
            <pre><code>sbatch my_job_script.sh</code></pre>
        </ol>
        
        <h2>Examples</h2>
        <p>Below are links to practical examples of parallel computing applications you can explore:</p>
        <ul>
            <li><a href="https://github.com/fasrc/User_Codes/tree/master/Parallel_Computing/MPI/Example1">Example 1: Monte Carlo Integration</a> - A demonstration of using MPI for parallel computation of integrals.</li>
            <li><a href="https://github.com/fasrc/User_Codes/tree/master/Parallel_Computing/MPI/Example2">Example 2: Matrix Multiplication</a> - An example illustrating parallel matrix operations using MPI.</li>
            <li><a href="https://github.com/fasrc/User_Codes/tree/master/Parallel_Computing/MPI/Example3">Example 3: Parallel File I/O</a> - This example shows how to manage file input and output in parallel.</li>
        </ul>
        
        <h2>References</h2>
        <p>For further reading and resources related to parallel computing and the FASRC cluster, please consult the following:</p>
        <ul>
            <li><a href="https://www.open-mpi.org/doc/v5.0/">Open MPI Documentation</a></li>
            <li><a href="https://www.mpich.org/documentation/">MPICH Documentation</a></li>
            <li><a href="https://docs.rc.fas.harvard.edu/kb/using-slurm/">SLURM Job Scheduler Documentation</a></li>
        </ul>
    </article>
</body>
</html>