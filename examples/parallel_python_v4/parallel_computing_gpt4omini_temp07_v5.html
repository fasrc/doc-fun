<html>
<head>
    <title>Parallel Computing on FASRC Cluster</title>
</head>
<body>
    <h1>Parallel Computing on the FASRC Cluster</h1>
    
    <h2>Description</h2>
    <p>Parallel computing is a computational paradigm that enables the simultaneous execution of multiple calculations or processes, leveraging multiple processors or cores to enhance performance and efficiency. In high-performance computing (HPC) environments, such as the Faculty Arts and Sciences Research Computing (FASRC) cluster, parallel computing is crucial for handling large-scale data analysis, complex simulations, and other resource-intensive tasks. The FASRC cluster supports various parallel computing frameworks, including OpenMPI and MPICH, facilitating the development and execution of parallel applications across distributed nodes.</p>
    
    <h2>Installation</h2>
    <p>To utilize parallel computing on the FASRC cluster, users must first load the appropriate MPI (Message Passing Interface) module depending on their specific application requirements. The recommended modules for MPI implementations on the FASRC cluster are:</p>
    <ul>
        <li><code>module load openmpi/4.1.4-fasrc01</code> - Open MPI implementation</li>
        <li><code>module load mpich/4.3.0-fasrc01</code> - MPICH MPI implementation (latest)</li>
        <li><code>module load mpich/4.2.2-fasrc01</code> - MPICH MPI implementation</li>
    </ul>
    <p>After loading the appropriate module, users can compile their parallel applications using the respective compiler flags, ensuring that the MPI library is correctly linked during the build process.</p>
    
    <h2>Usage</h2>
    <p>To effectively utilize parallel computing on the FASRC cluster, users should follow best practices for job submission and resource management. The SLURM workload manager is employed to manage job scheduling and resource allocation. Common SLURM commands include:</p>
    <ul>
        <li><code>sbatch script.sh</code> - Submit a batch job to the SLURM scheduler.</li>
        <li><code>squeue -u username</code> - View job queue status.</li>
        <li><code>scancel job_id</code> - Cancel submitted jobs.</li>
        <li><code>sinfo</code> - View cluster partition information.</li>
        <li><code>salloc --partition=shared --time=2:00:00</code> - Allocate an interactive session.</li>
        <li><code>srun command</code> - Run commands on allocated nodes.</li>
    </ul>
    <p>Users should also consider data locality and communication overhead when designing their parallel applications, as these factors can significantly impact performance. It is recommended to optimize data access patterns and minimize inter-process communication where possible.</p>
    
    <h2>Examples</h2>
    <p>Here are several practical examples illustrating the use of parallel computing on the FASRC cluster:</p>
    <ul>
        <li><a href="src/User_Codes/Parallel_Computing/R/Large_Data_Processing_R/parallel_computation/R_embarrassingly_parallel.md">R Embarrassingly Parallel</a>: This example demonstrates scaling up/out embarrassingly parallel problems in R, utilizing multiple cores for enhanced performance.</li>
        <li><a href="src/User_Codes/Parallel_Computing/Parallel_HDF5/Example1/parallel_hdf5.f90">Parallel HDF5</a>: A Fortran program that creates a random vector and distributes it to multiple processes for parallel file I/O operations.</li>
        <li><a href="src/User_Codes/Parallel_Computing/Parallel_HDF5/Example2/parallel_hdf5_2d.f90">Parallel HDF5 2D</a>: This example generates a random 2D array and writes it in parallel using HDF5.</li>
        <li><a href="src/User_Codes/Parallel_Computing/MATLAB/Example1/parallel_monte_carlo.m">Parallel Monte Carlo</a>: An Objective-C example that performs a parallel Monte Carlo calculation of Ï€, illustrating the effectiveness of parallel computation in statistical simulations.</li>
    </ul>
    
    <h2>References</h2>
    <p>For additional resources and information regarding parallel computing and MPI, please refer to the following:</p>
    <ul>
        <li><a href="https://www.open-mpi.org/">Open MPI Official Website</a></li>
        <li><a href="https://www.mpich.org/">MPICH Official Website</a></li>
        <li><a href="https://docs.rc.fas.harvard.edu/kb/access-and-login/">FASRC Access and Login Documentation</a></li>
        <li><a href="https://docs.rc.fas.harvard.edu/kb/running-jobs/">Running Jobs on the FASRC Cluster</a></li>
    </ul>
</body>
</html>