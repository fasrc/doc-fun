<html>
<head>
    <title>Parallel Computing on the FASRC Cluster</title>
</head>
<body>
    <article>
        <h1>Parallel Computing on the FASRC Cluster</h1>
        
        <h2>Description</h2>
        <p>Parallel computing is a computational paradigm that enables the execution of multiple processes simultaneously, leveraging the power of multiple CPUs or cores. This approach is essential for handling large-scale computations and data-intensive tasks typical in research and scientific computing. The Faculty Arts and Sciences Research Computing (FASRC) cluster provides a robust environment for parallel computing, supporting various parallel programming models and libraries, including MPI (Message Passing Interface) and OpenMP (Open Multi-Processing).</p>

        <h2>Installation</h2>
        <p>To begin utilizing parallel computing on the FASRC cluster, it is necessary to load the appropriate software modules that support the desired parallel programming model. Below are the recommended modules for MPI and OpenMP:</p>
        <pre>
        module load openmpi/4.1.4-fasrc01  # For MPI applications
        module load mpich/4.3.0-fasrc01    # Alternative MPI implementation
        </pre>
        <p>For OpenMP, you can typically use the standard compiler options with GCC or Intel compilers:</p>
        <pre>
        module load gcc/10.2.0-fasrc01      # Load GCC for OpenMP
        module load intel/19.1.3-fasrc01    # Load Intel compiler for OpenMP
        </pre>
        <p>Ensure that you have the necessary permissions and environment set up correctly for executing parallel jobs on the cluster.</p>

        <h2>Usage</h2>
        <p>To utilize parallel computing effectively, it is essential to structure your code to take advantage of multiple processors. This can be achieved through the use of directives (for OpenMP) or message-passing calls (for MPI). Below are basic usage patterns:</p>
        
        <h3>Using OpenMP</h3>
        <pre>
        #pragma omp parallel for
        for (int i = 0; i < N; i++) {
            // Perform computation
        }
        </pre>
        
        <h3>Using MPI</h3>
        <pre>
        MPI_Init(&argc, &argv);
        MPI_Comm_size(MPI_COMM_WORLD, &size);
        MPI_Comm_rank(MPI_COMM_WORLD, &rank);
        // Perform parallel computation
        MPI_Finalize();
        </pre>

        <h2>Examples</h2>
        <p>Here are some practical examples of parallel computing applications that can be executed on the FASRC cluster:</p>
        <ul>
            <li><a href="https://github.com/fasrc/User_Codes/tree/master/Parallel_Computing/MPI/Example1">Example 1</a>: Monte Carlo simulation for estimating Ï€.</li>
            <li><a href="https://github.com/fasrc/User_Codes/tree/master/Parallel_Computing/MPI/Example2">Example 2</a>: Parallel integration of a mathematical function using trapezoidal rule.</li>
            <li><a href="https://github.com/fasrc/User_Codes/tree/master/Parallel_Computing/OpenMP/Example1">Example 3</a>: Matrix multiplication using OpenMP.</li>
        </ul>

        <h2>References</h2>
        <p>For further information on parallel computing, please consult the following resources:</p>
        <ul>
            <li><a href="https://www.openmp.org">OpenMP Official Website</a></li>
            <li><a href="https://www.mpich.org">MPICH Official Website</a></li>
            <li><a href="https://docs.rc.fas.harvard.edu">FASRC Documentation</a></li>
        </ul>
    </article>
</body>
</html>