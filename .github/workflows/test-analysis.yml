name: Test Analysis with GitHub Integration

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:
  schedule:
    # Run weekly test analysis on Sundays at 2 AM UTC
    - cron: '0 2 * * 0'

jobs:
  test-analysis:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.9, 3.10, 3.11]
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
        
    - name: Cache dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements*.txt', '**/pyproject.toml') }}
        restore-keys: |
          ${{ runner.os }}-pip-
          
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e .[test,dev]
        
    - name: Run comprehensive test analysis
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      run: |
        python -m doc_generator.github_test_runner \
          --test-path tests/ \
          --workspace . \
          --github-token "$GITHUB_TOKEN" \
          --performance-threshold 3.0 \
          ${{ github.event_name == 'pull_request' && format('--pr-number {0}', github.event.pull_request.number) || '' }}
      
    - name: Upload test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: test-results-python${{ matrix.python-version }}
        path: |
          test-results.xml
          coverage.xml
          test-analysis.json
        retention-days: 30
        
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      if: matrix.python-version == '3.10'
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella
        fail_ci_if_error: false

  integration-tests:
    runs-on: ubuntu-latest
    needs: test-analysis
    if: github.event_name == 'pull_request'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: 3.10
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e .[test,dev]
        
    - name: Run integration tests with analysis
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      run: |
        python -c "
        from doc_generator.github_test_runner import run_github_tests
        
        # Run integration tests with custom configuration
        analysis = run_github_tests(
            test_path='tests/integration/',
            enable_coverage=True,
            performance_threshold=10.0,  # Integration tests can be slower
            save_results=True,
            post_summary=True
        )
        
        # Custom analysis for integration tests
        print(f'Integration test success rate: {analysis.suite_results.success_rate:.1f}%')
        
        if analysis.critical_issues:
            print('Critical issues found:')
            for issue in analysis.critical_issues:
                print(f'  - {issue}')
        
        # Fail if integration tests have critical issues
        if analysis.critical_issues:
            exit(1)
        "
        
  performance-tests:
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: 3.10
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e .[test,dev]
        pip install pytest-benchmark
        
    - name: Run performance tests with analysis
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      run: |
        python -c "
        from doc_generator.github_test_runner import GitHubTestRunner
        import logging
        
        logging.basicConfig(level=logging.INFO)
        
        # Custom runner for performance tests
        runner = GitHubTestRunner(
            workspace_path='.',
            github_token='${{ secrets.GITHUB_TOKEN }}',
            enable_coverage=False,  # Skip coverage for performance tests
            performance_threshold=1.0  # Stricter threshold for performance tests
        )
        
        # Run performance-focused tests
        suite_result = runner.run_tests(
            test_path='tests/performance/',
            extra_args=['--benchmark-only', '--benchmark-json=benchmark-results.json']
        )
        
        # Analyze with focus on performance
        analysis = runner.analyze_results(suite_result)
        
        # Generate performance report
        summary = runner.generate_github_summary(analysis)
        
        # Add benchmark analysis if available
        import json
        import os
        
        if os.path.exists('benchmark-results.json'):
            with open('benchmark-results.json') as f:
                benchmarks = json.load(f)
            
            summary += '\n### 📊 Performance Benchmarks\n'
            for benchmark in benchmarks.get('benchmarks', []):
                name = benchmark.get('name', 'Unknown')
                mean = benchmark.get('stats', {}).get('mean', 0)
                summary += f'- **{name}**: {mean:.4f}s average\n'
        
        # Write enhanced summary
        summary_file = os.getenv('GITHUB_STEP_SUMMARY')
        if summary_file:
            with open(summary_file, 'w') as f:
                f.write(summary)
        else:
            print(summary)
        "
        
    - name: Upload benchmark results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: performance-benchmarks
        path: |
          benchmark-results.json
          test-analysis.json
        retention-days: 90

  security-tests:
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request' || github.event_name == 'push'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: 3.10
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e .[test,dev]
        pip install bandit safety
        
    - name: Run security tests with analysis
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      run: |
        # Run security scans
        bandit -r src/ -f json -o bandit-results.json || true
        safety check --json --output safety-results.json || true
        
        python -c "
        from doc_generator.github_test_runner import GitHubTestRunner
        import json
        import os
        
        runner = GitHubTestRunner(
            workspace_path='.',
            github_token='${{ secrets.GITHUB_TOKEN }}',
            enable_coverage=False
        )
        
        # Create custom security test results
        security_issues = []
        
        # Parse Bandit results
        if os.path.exists('bandit-results.json'):
            with open('bandit-results.json') as f:
                bandit_data = json.load(f)
                security_issues.extend([
                    f'Security issue: {result.get(\"test_name\", \"Unknown\")} in {result.get(\"filename\", \"Unknown\")}'
                    for result in bandit_data.get('results', [])
                ])
        
        # Parse Safety results  
        if os.path.exists('safety-results.json'):
            with open('safety-results.json') as f:
                try:
                    safety_data = json.load(f)
                    security_issues.extend([
                        f'Vulnerable dependency: {vuln.get(\"package_name\", \"Unknown\")} {vuln.get(\"analyzed_version\", \"\")}'
                        for vuln in safety_data if isinstance(safety_data, list)
                    ])
                except json.JSONDecodeError:
                    pass  # Safety sometimes returns non-JSON output
        
        # Generate security summary
        summary = f'''## 🛡️ Security Analysis Results
        
### 📊 Security Issues Found: {len(security_issues)}

'''
        
        if security_issues:
            summary += '### 🚨 Issues Detected\n'
            for issue in security_issues[:10]:  # Limit to first 10
                summary += f'- {issue}\n'
            
            if len(security_issues) > 10:
                summary += f'- ... and {len(security_issues) - 10} more issues\n'
        else:
            summary += '✅ No security issues detected!\n'
        
        # Write security summary
        summary_file = os.getenv('GITHUB_STEP_SUMMARY')
        if summary_file:
            with open(summary_file, 'a') as f:  # Append to existing summary
                f.write('\n---\n' + summary)
        else:
            print(summary)
        
        # Fail if critical security issues found
        critical_security_issues = [issue for issue in security_issues if 'high' in issue.lower() or 'critical' in issue.lower()]
        if critical_security_issues:
            print(f'Found {len(critical_security_issues)} critical security issues')
            exit(1)
        "
        
    - name: Upload security results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: security-results
        path: |
          bandit-results.json
          safety-results.json
        retention-days: 30

  test-quality-gate:
    runs-on: ubuntu-latest
    needs: [test-analysis, integration-tests]
    if: always() && github.event_name == 'pull_request'
    
    steps:
    - name: Download test results
      uses: actions/download-artifact@v3
      with:
        name: test-results-python3.10
        
    - name: Quality gate analysis
      run: |
        python -c "
        import json
        import sys
        
        # Load test analysis results
        with open('test-analysis.json') as f:
            analysis = json.load(f)
        
        suite = analysis['suite_results']
        success_rate = suite['passed'] / suite['total_tests'] * 100 if suite['total_tests'] > 0 else 0
        coverage = suite.get('coverage_percent', 0)
        
        print(f'Success Rate: {success_rate:.1f}%')
        print(f'Coverage: {coverage:.1f}%')
        
        # Quality gate criteria
        min_success_rate = 95.0
        min_coverage = 80.0
        
        # Check quality gates
        quality_gates_passed = True
        
        if success_rate < min_success_rate:
            print(f'❌ FAIL: Success rate {success_rate:.1f}% is below minimum {min_success_rate}%')
            quality_gates_passed = False
        else:
            print(f'✅ PASS: Success rate {success_rate:.1f}% meets minimum {min_success_rate}%')
        
        if coverage > 0 and coverage < min_coverage:
            print(f'❌ FAIL: Coverage {coverage:.1f}% is below minimum {min_coverage}%')
            quality_gates_passed = False
        else:
            print(f'✅ PASS: Coverage {coverage:.1f}% meets minimum {min_coverage}%')
        
        # Check for critical issues
        critical_issues = analysis.get('critical_issues', [])
        if critical_issues:
            print(f'❌ FAIL: {len(critical_issues)} critical issues found')
            quality_gates_passed = False
        else:
            print('✅ PASS: No critical issues found')
        
        if not quality_gates_passed:
            print('\\n🚫 Quality gates failed. PR cannot be merged.')
            sys.exit(1)
        else:
            print('\\n✅ All quality gates passed. PR is ready for review.')
        "

  notify-teams:
    runs-on: ubuntu-latest
    needs: [test-analysis, integration-tests, security-tests]
    if: always() && (failure() || github.event_name == 'schedule')
    
    steps:
    - name: Send notification
      run: |
        # This would typically send notifications to Slack, Teams, etc.
        echo "Test analysis complete. Results available in job summaries."
        
        # Example webhook notification (replace with your actual webhook)
        # curl -X POST -H 'Content-type: application/json' \
        #   --data '{"text":"GitHub test analysis complete for ${{ github.repository }}"}' \
        #   ${{ secrets.SLACK_WEBHOOK_URL }}