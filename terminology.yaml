# terminology.yaml
# HPC Environment Terminology for Documentation Generation
# Seeded with modules from FASRC Cannon cluster

hpc_modules:
  # Programming Languages & Environments
  - name: "python/3.12.8-fasrc01"
    description: "Python 3.12 with Anaconda distribution"
    category: "programming"
  - name: "python/3.10.13-fasrc01"
    description: "Python 3.10 with Anaconda distribution"
    category: "programming"
  - name: "R/4.4.3-fasrc01"
    description: "R statistical computing environment"
    category: "programming"
  - name: "R/4.3.3-fasrc01"
    description: "R statistical computing environment"
    category: "programming"
  - name: "julia/1.0.0-ncf"
    description: "Julia high-performance computing language"
    category: "programming"
  - name: "matlab/R2024b-fasrc01"
    description: "MATLAB technical computing platform"
    category: "programming"
  - name: "mathematica/13.3.0-fasrc01"
    description: "Mathematica computational software"
    category: "programming"
  - name: "Anaconda2/2019.10-fasrc01"
    description: "Anaconda Python2 distribution"
    category: "programming"
  - name: "Mambaforge/23.11.0-fasrc01"
    description: "Mamba Python implementation"
    category: "programming"
  - name: "Miniforge3/24.11.3-fasrc02"
    description: "Miniforge Python implementation"
    category: "programming"
  - name: "intelpython/3.9.16-fasrc01"
    description: "Intel distribution of Python"
    category: "programming"

  # Compilers
  - name: "gcc/14.2.0-fasrc01"
    description: "GNU Compiler Collection (latest)"
    category: "compiler"
  - name: "gcc/13.2.0-fasrc01"
    description: "GNU Compiler Collection"
    category: "compiler"
  - name: "gcc/12.2.0-fasrc01"
    description: "GNU Compiler Collection"
    category: "compiler"
  - name: "intel/25.0.1-fasrc01"
    description: "Intel OneAPI compilers and tools (latest)"
    category: "compiler"
  - name: "intel/24.2.1-fasrc01"
    description: "Intel OneAPI compilers and tools"
    category: "compiler"
  - name: "nvhpc/24.11-fasrc01"
    description: "NVIDIA HPC SDK compilers"
    category: "compiler"

  # GPU Computing
  - name: "cuda/12.9.1-fasrc01"
    description: "NVIDIA CUDA toolkit (latest)"
    category: "gpu"
  - name: "cuda/12.4.1-fasrc01"
    description: "NVIDIA CUDA toolkit"
    category: "gpu"
  - name: "cuda/11.8.0-fasrc01"
    description: "NVIDIA CUDA toolkit (older stable)"
    category: "gpu"
  - name: "cudnn/9.5.1.17_cuda12-fasrc01"
    description: "NVIDIA cuDNN deep learning library (latest)"
    category: "gpu"
  - name: "cudnn/8.9.2.26_cuda12-fasrc01"
    description: "NVIDIA cuDNN deep learning library"
    category: "gpu"

  # Parallel Computing & MPI
  - name: "openmpi/4.1.5-fasrc03"
    description: "Open MPI implementation (latest)"
    category: "parallel"
  - name: "openmpi/4.1.4-fasrc01"
    description: "Open MPI implementation"
    category: "parallel"
  - name: "mpich/4.3.0-fasrc01"
    description: "MPICH MPI implementation (latest)"
    category: "parallel"
  - name: "mpich/4.2.2-fasrc01"
    description: "MPICH MPI implementation"
    category: "parallel"
  - name: "intelmpi/2021.14-fasrc01"
    description: "Intel MPI library (latest)"
    category: "parallel"
  - name: "intelmpi/2021.13-fasrc01"
    description: "Intel MPI library"
    category: "parallel"

  # Scientific Libraries
  - name: "fftw/3.3.10-fasrc01"
    description: "Fastest Fourier Transform in the West"
    category: "library"
  - name: "hdf5/1.14.2-fasrc02"
    description: "HDF5 data model and file format (latest)"
    category: "library"
  - name: "hdf5/1.12.2-fasrc01"
    description: "HDF5 data model and file format"
    category: "library"
  - name: "netcdf-c/4.9.2-fasrc05"
    description: "NetCDF C libraries (latest)"
    category: "library"
  - name: "netcdf-fortran/4.6.1-fasrc01"
    description: "NetCDF Fortran libraries"
    category: "library"
  - name: "netcdf-cxx4/4.3.1-fasrc04"
    description: "NetCDF C++ libraries"
    category: "library"
  - name: "gsl/2.8-fasrc01"
    description: "GNU Scientific Library"
    category: "library"
  - name: "intel-mkl/25.0.1-fasrc01"
    description: "Intel Math Kernel Library (latest)"
    category: "library"
  - name: "openblas/0.3.21-fasrc01"
    description: "Optimized BLAS library"
    category: "library"

  # Development Tools
  - name: "cmake/3.31.6-fasrc01"
    description: "Cross platform build tool (latest)"
    category: "development"
  - name: "cmake/3.30.3-fasrc01"
    description: "Cross platform build tool"
    category: "development"
  - name: "autoconf/2.71-fasrc01"
    description: "GNU Autoconf build system"
    category: "development"
  - name: "flex/2.6.4-fasrc01"
    description: "Lexical analyzer generator"
    category: "development"
  - name: "jdk/23.0.2-fasrc01"
    description: "Java Development Kit (latest)"
    category: "development"

  # Computational Chemistry & Physics
  - name: "gaussian/16-fasrc04"
    description: "Gaussian computational chemistry software"
    category: "chemistry"
  - name: "QChem/6.1-fasrc01"
    description: "Q-Chem quantum chemistry package"
    category: "chemistry"
  - name: "comsol/6.3-fasrc01"
    description: "COMSOL Multiphysics simulation"
    category: "physics"
  - name: "abaqus/2023-fasrc01"
    description: "Abaqus finite element analysis"
    category: "physics"
  - name: "lumerical-seas/2024R1-fasrc01"
    description: "Lumerical FDTD simulation software"
    category: "physics"

  # Data Science & Visualization
  - name: "rstudio/2024.12.1-fasrc01"
    description: "RStudio IDE for R (latest)"
    category: "datascience"
  - name: "vscode/1.98-fasrc01"
    description: "Visual Studio Code editor (latest)"
    category: "datascience"
  - name: "pycharm-community/2023.1-fasrc01"
    description: "PyCharm Python IDE"
    category: "datascience"
  - name: "knime/5.4.4-fasrc01"
    description: "KNIME data analytics platform"
    category: "datascience"
  - name: "sage/10.3-fasrc01"
    description: "SageMath mathematical software"
    category: "datascience"

  # Optimization & Mathematical Software
  - name: "gurobi/12.0.1-fasrc01"
    description: "Gurobi optimization solver"
    category: "optimization"
  - name: "knitro/13.2.0-fasrc01"
    description: "KNITRO nonlinear optimization solver"
    category: "optimization"
  - name: "magma/2.28.7-fasrc02"
    description: "Magma computational algebra system"
    category: "mathematics"
  - name: "pari/2.15.5-fasrc01"
    description: "PARI/GP number theory system"
    category: "mathematics"

cluster_commands:
  - name: "sbatch"
    description: "Submit batch job to SLURM scheduler"
    usage: "sbatch script.sh"
    example: "sbatch --partition=gpu --gres=gpu:1 my_job.sh"
  - name: "squeue"
    description: "View job queue status"
    usage: "squeue -u username"
    example: "squeue -u $USER --format=\"%.18i %.9P %.30j %.8u %.8T %.10M %.9l %.6D %R\""
  - name: "scancel"
    description: "Cancel submitted jobs"
    usage: "scancel job_id"
    example: "scancel 12345"
  - name: "sinfo"
    description: "View cluster partition information"
    usage: "sinfo"
    example: "sinfo -p gpu --format=\"%20P %5a %10l %6t %6c %8z %7m %8d %19N\""
  - name: "salloc"
    description: "Allocate interactive session"
    usage: "salloc --partition=shared --time=2:00:00"
    example: "salloc --partition=gpu --gres=gpu:1 --time=4:00:00 --mem=16G"
  - name: "srun"
    description: "Run commands on allocated nodes"
    usage: "srun command"
    example: "srun --pty bash"
  - name: "module"
    description: "Environment module system"
    usage: "module load/unload/list/avail/spider"
    example: "module load python/3.12.8-fasrc01 cuda/12.9.1-fasrc01"

filesystems:
  - name: "/n/holyscratch01"
    description: "High-performance scratch storage (30-day purge)"
    type: "scratch"
    usage: "Temporary files, large datasets"
    quota: "100TB default"
  - name: "/n/home_lab"
    description: "Lab group home directories"
    type: "home"
    usage: "Source code, small datasets"
    quota: "50GB default"
  - name: "/n/holylabs"
    description: "Lab group shared storage"
    type: "shared"
    usage: "Persistent shared data"
    quota: "Varies by lab"
  - name: "/n/holylfs"
    description: "Lustre high-performance filesystem"
    type: "performance"
    usage: "Large-scale parallel I/O"
    quota: "By allocation"

partitions:
  - name: "shared"
    description: "Shared CPU nodes for general computing"
    max_time: "30-00:00:00"
    max_nodes: "1"
    cores_per_node: "48"
    memory_per_node: "192GB"
  - name: "gpu"
    description: "GPU nodes with V100/A100 accelerators"
    max_time: "7-00:00:00"
    max_nodes: "4"
    gpu_types: ["V100", "A100"]
    memory_per_node: "384GB"
  - name: "gpu_test"
    description: "GPU nodes for testing (shorter jobs)"
    max_time: "00:30:00"
    max_nodes: "1"
    gpu_types: ["V100", "A100"]
    memory_per_node: "384GB"
  - name: "bigmem"
    description: "High-memory nodes for memory-intensive jobs"
    max_time: "7-00:00:00"
    max_nodes: "2"
    cores_per_node: "48"
    memory_per_node: "1TB"
  - name: "serial_requeue"
    description: "Single-core jobs with preemption"
    max_time: "30-00:00:00"
    max_nodes: "1"
    preemptible: true
    cores_per_node: "1"

job_examples:
  basic_cpu:
    description: "Basic CPU job submission"
    script: |
      #!/bin/bash
      #SBATCH --job-name=my_job
      #SBATCH --partition=shared
      #SBATCH --time=4:00:00
      #SBATCH --ntasks=1
      #SBATCH --cpus-per-task=4
      #SBATCH --mem=16G
      #SBATCH --output=%j.out
      #SBATCH --error=%j.err
      
      module load python/3.12.8-fasrc01
      python my_script.py
  
  gpu_job:
    description: "GPU job with CUDA"
    script: |
      #!/bin/bash
      #SBATCH --job-name=gpu_job
      #SBATCH --partition=gpu
      #SBATCH --gres=gpu:1
      #SBATCH --time=2:00:00
      #SBATCH --ntasks=1
      #SBATCH --cpus-per-task=8
      #SBATCH --mem=32G
      #SBATCH --output=%j.out
      #SBATCH --error=%j.err
      
      module load python/3.12.8-fasrc01 cuda/12.9.1-fasrc01
      python gpu_training.py
  
  mpi_job:
    description: "Multi-node MPI job"
    script: |
      #!/bin/bash
      #SBATCH --job-name=mpi_job
      #SBATCH --partition=shared
      #SBATCH --nodes=2
      #SBATCH --ntasks-per-node=24
      #SBATCH --time=8:00:00
      #SBATCH --mem-per-cpu=4G
      #SBATCH --output=%j.out
      #SBATCH --error=%j.err
      
      module load gcc/14.2.0-fasrc01 openmpi/4.1.5-fasrc03
      mpirun ./my_mpi_program

common_workflows:
  interactive_session:
    description: "Start interactive computing session"
    steps:
      - "salloc --partition=shared --time=2:00:00 --mem=8G"
      - "module load python/3.12.8-fasrc01"
      - "python"
  
  gpu_interactive:
    description: "Interactive GPU session for development"
    steps:
      - "salloc --partition=gpu_test --gres=gpu:1 --time=30:00 --mem=16G"
      - "module load python/3.12.8-fasrc01 cuda/12.9.1-fasrc01"
      - "nvidia-smi"
  
  file_transfer:
    description: "Transfer files to/from cluster"
    steps:
      - "# From local to cluster:"
      - "scp file.txt username@login.rc.fas.harvard.edu:/n/home_user/"
      - "# From cluster to local:"
      - "scp username@login.rc.fas.harvard.edu:/path/to/file.txt ."

best_practices:
  - "Always specify resource requirements accurately in SLURM scripts"
  - "Use scratch storage (/n/holyscratch01) for temporary files and large datasets"
  - "Load only necessary modules to avoid conflicts"
  - "Test jobs interactively before submitting large batch jobs"
  - "Monitor job progress with squeue and check resource usage"
  - "Clean up scratch files regularly to maintain good cluster citizenship"
  - "Use array jobs for parameter sweeps and multiple similar tasks"
  - "Request appropriate walltime - too short kills jobs, too long delays scheduling"
  - "Use module spider to find detailed information about specific modules"
  - "Check module dependencies and conflicts before loading multiple modules"