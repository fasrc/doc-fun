<html><body><div><article class="markdown-body entry-content container-lg">
<div class="markdown-heading" dir="auto">
<h2 class="font-600 text-2xl font-bold"><span id="Introduction">Introduction</span></h2>
<p dir="auto">The Message Passing Interface (MPI) library allows processes in your parallel application to communicate with one another by sending and receiving messages. There is no default MPI library in your environment when you log in to the cluster. You need to choose the desired MPI implementation for your applications. This is done by loading an appropriate MPI module. Currently the available MPI implementations on our cluster are <a href="https://www.open-mpi.org/" rel="nofollow">OpenMPI</a> and <a href="https://www.mpich.org/" rel="nofollow">Mpich</a>. For both implementations the MPI libraries are compiled and built with either the <a href="https://www.intel.com/content/www/us/en/developer/tools/oneapi/toolkits.html" rel="nofollow">Intel compiler suite</a> or the <a href="https://www.gnu.org/software/gcc/" rel="nofollow">GNU compiler suite</a>. These are organized in <a href="https://docs.rc.fas.harvard.edu/kb/modules-intro/" rel="nofollow">software modules</a>.</p>
<h2 dir="auto"><span id="Installation">Installation</span></h2>
<p>MPI has many forms, we’ll list a few here, and also look at <a href="https://github.com/fasrc/User_Codes/tree/master/Parallel_Computing/MPI">User_Codes/Parallel_Computing/MPI</a>.</p>
<h3><span id="mpi4py_with_Python">mpi4py with Python</span></h3>
<p>To use <a href="https://pythonhosted.org/mpi4py" target="_blank" rel="noopener noreferrer"><span><b>mpi4py</b></span></a> you need to load an appropriate Python software module. We have the <a href="https://pythonhosted.org/mpi4py" target="_blank" rel="noopener noreferrer"><span><b>Anaconda</b></span></a> Python distribution from Continuum Analytics. In addition to mpi4py, it includes hundreds of the most popular packages for large-scale data processing and scientific computing.</p>
<p>You can load python in your user environment by running in your terminal:</p>
<pre>module load python/2.7.14-fasrc01</pre>
<p>For example code, see <a href="https://github.com/fasrc/User_Codes/tree/57c501d3a5925f81803f0c53ffba491be54c4c3b/Parallel_Computing/Python/mpi4py">Parallel_Computing/Python/mpi4py</a></p>
<h3><span id="OpenMPI_with_GNU_Compiler"><span>OpenMPI with GNU Compiler</span></span></h3>
</div>
</article>
<article class="markdown-body entry-content container-lg">
<div class="markdown-heading" dir="auto">
<p dir="auto">If you want to use OpenMPI compiled with the GNU compiler you need to load appropriate compiler and MPI modules. Below are some possible combinations, check <code>module spider MODULENAME</code> to get a full listing of possibilities.</p>
<div class="highlight highlight-source-shell notranslate position-relative overflow-auto" dir="auto">
<pre><span class="pl-c"># GCC + OpenMPI, e.g.,</span>
module load gcc/13.2.0-fasrc01 openmpi/5.0.2-fasrc01

<span class="pl-c"># GCC + Mpich, e.g.,</span>
module load gcc/13.2.0-fasrc01 mpich/4.2.0-fasrc01

<span class="pl-c"># Intel + OpenMPI, e.g.,</span>
module load intel/24.0.1-fasrc01 openmpi/5.0.2-fasrc01

<span class="pl-c"># Intel + Mpich, e.g.,</span>
module load intel/24.0.1-fasrc01 mpich/4.2.0-fasrc01

<span class="pl-c"># Intel + IntelMPI (IntelMPI runs mpich underneath), e.g.</span>
module load intel/24.0.1-fasrc01 intelmpi/2021.11-fasrc01</pre>
</div>
<p dir="auto">For reproducibility and consistency it is recommended to use the complete module name with the module load command, as illustrated above. Modules on the cluster get updated often so check if there are more recent ones. The modules are set up so that you can only have one MPI module loaded at a time. If you try loading a second one it will automatically unload the first. This is done to avoid dependencies collisions.</p>
<p dir="auto">There are four ways you can set up your MPI on the cluster:</p>
<ul dir="auto">
<li>
<p dir="auto">Put the module load command in your startup files.<br>
Most users will find this option most convenient. You will likely only want to use a single version of MPI for all your work. This method also works with all MPI modules currently available on the cluster.</p>
</li>
<li>
<p dir="auto">Load the module in your current shell.<br>
For the current MPI versions you do not need to have the module load command in your startup files. If you submit a job the remote processes will inherit the submission shell environment and use the proper MPI library. Note this method does not work with older versions of MPI.</p>
</li>
<li>
<p dir="auto">Load the module in your job script.<br>
If you will be using different versions of MPI for different jobs, then you can put the module load command in your script. You need to ensure your script can execute the module load command properly.</p>
</li>
<li>
<p dir="auto">Do not use modules and set environment variables yourself.<br>
You obviously do not need to use modules but can hard code paths. However, these locations may change without warning so you should set them in one location only and not scatter them throughout your scripts. This option could be useful if you have a customized local build of MPI you would like to use with your applications.</p>
</li>
</ul>
<h2 dir="auto"><span id="Video_Training">Video Training</span></h2>
<p class="jetpack-video-wrapper"><iframe title="Parallel Job Workflows: Running OpenMP, MPI and Hybrid OpenMP+MPI jobs on Harvard's Cannon cluster" src="https://www.youtube.com/embed/_8ap3sMS3nU?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen>VIDEO</iframe></p>
</div>
<p class="markdown-heading" dir="auto">
<h2 class="heading-element" dir="auto" tabindex="-1"><span id="Examples">Examples</span></h2>
</p>
<p dir="auto">For associated MPI examples, head over to  <a href="https://github.com/fasrc/User_Codes/tree/master/Parallel_Computing/MPI" target="_blank" rel="noopener">User_Codes/Parallel_Computing/MPI</a>.</p>
<ul dir="auto">
<li><strong><a href="https://github.com/fasrc/User_Codes/tree/master/Parallel_Computing/MPI/Example1">Example 1</a>:</strong> Monte-Carlo calculation of π</li>
<li><strong><a href="https://github.com/fasrc/User_Codes/tree/master/Parallel_Computing/MPI/Example2">Example 2</a>:</strong> Integration of x2 in interval [0, 4] with 80 integration points and the trapezoidal rule</li>
<li><strong><a href="https://github.com/fasrc/User_Codes/tree/master/Parallel_Computing/MPI/Example3">Example 3</a>:</strong> Parallel Lanczos diagonalization with reorthogonalization and MPI I/O</li>
</ul>
<p class="markdown-heading" dir="auto">
<h2 class="heading-element" dir="auto" tabindex="-1"><span id="Resources">Resources</span></h2>
</p>

</article>
</div></body></html>